{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL\n",
    "\n",
    "In this notebook we will transform the raw data that we explored in the previous one. The target of our problem, for each day, is the amount of the next bill that the reseller will pay. \n",
    "\n",
    "As the model is going to need to run every day to keep predictions relevant, the ETL that we build here is going to be deployed in Glue as a process that we are going to run every day orchestrated by Step Functions.\n",
    "\n",
    "The Glue process is going to read the raw data from the Data Lake, and is going to take the last 4 months of history to build features relevant to predict how much is each reseller going to expend next time, based on reseller's characteristics and past shopping patterns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import boto3\n",
    "import pickle\n",
    "import io\n",
    "import awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'zoomagri-maxi-bucket-sagemaker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'bucket' (str)\n"
     ]
    }
   ],
   "source": [
    "%store bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = awswrangler.Session()\n",
    "df = session.pandas.read_sql_athena(\n",
    "    sql=\"select * from resellers_sample\",\n",
    "    database=\"implementationdb\"\n",
    ")\n",
    "df_r = session.pandas.read_sql_athena(\n",
    "    sql=\"select * from reseller\",\n",
    "    database=\"implementationdb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id_reseller</th>\n",
       "      <th>bill</th>\n",
       "      <th>mean-last-30</th>\n",
       "      <th>mean-last-7</th>\n",
       "      <th>std-last-30</th>\n",
       "      <th>days_without_purchase</th>\n",
       "      <th>weekday</th>\n",
       "      <th>next_bill</th>\n",
       "      <th>last_bill</th>\n",
       "      <th>zone</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-31</td>\n",
       "      <td>499921276</td>\n",
       "      <td>10515.048</td>\n",
       "      <td>9249.553500</td>\n",
       "      <td>9249.55350</td>\n",
       "      <td>1789.679485</td>\n",
       "      <td>2</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>10515.048</td>\n",
       "      <td>7984.059</td>\n",
       "      <td>1050</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>499921276</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9249.553500</td>\n",
       "      <td>9249.55350</td>\n",
       "      <td>1789.679485</td>\n",
       "      <td>0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>10717.103</td>\n",
       "      <td>10515.048</td>\n",
       "      <td>1050</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-02-02</td>\n",
       "      <td>499921276</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9249.553500</td>\n",
       "      <td>9249.55350</td>\n",
       "      <td>1789.679485</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>10717.103</td>\n",
       "      <td>10515.048</td>\n",
       "      <td>1050</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-02-03</td>\n",
       "      <td>499921276</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9249.553500</td>\n",
       "      <td>9249.55350</td>\n",
       "      <td>1789.679485</td>\n",
       "      <td>2</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>10717.103</td>\n",
       "      <td>10515.048</td>\n",
       "      <td>1050</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>499921276</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9249.553500</td>\n",
       "      <td>10515.04800</td>\n",
       "      <td>1789.679485</td>\n",
       "      <td>3</td>\n",
       "      <td>Monday</td>\n",
       "      <td>10717.103</td>\n",
       "      <td>10515.048</td>\n",
       "      <td>1050</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139880</th>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>500800672</td>\n",
       "      <td>9741.581</td>\n",
       "      <td>11722.998800</td>\n",
       "      <td>12917.17375</td>\n",
       "      <td>7280.688377</td>\n",
       "      <td>2</td>\n",
       "      <td>Monday</td>\n",
       "      <td>9741.581</td>\n",
       "      <td>1196.989</td>\n",
       "      <td>1050</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139881</th>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>500800672</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11722.998800</td>\n",
       "      <td>12795.63600</td>\n",
       "      <td>7280.688377</td>\n",
       "      <td>0</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>11711.968</td>\n",
       "      <td>9741.581</td>\n",
       "      <td>1050</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139882</th>\n",
       "      <td>2019-05-15</td>\n",
       "      <td>500800672</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11348.496444</td>\n",
       "      <td>12795.63600</td>\n",
       "      <td>7619.490861</td>\n",
       "      <td>1</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>11711.968</td>\n",
       "      <td>9741.581</td>\n",
       "      <td>1050</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139883</th>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>500800672</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11348.496444</td>\n",
       "      <td>5469.28500</td>\n",
       "      <td>7619.490861</td>\n",
       "      <td>2</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>11711.968</td>\n",
       "      <td>9741.581</td>\n",
       "      <td>1050</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139884</th>\n",
       "      <td>2019-05-17</td>\n",
       "      <td>500800672</td>\n",
       "      <td>11711.968</td>\n",
       "      <td>11384.843600</td>\n",
       "      <td>10726.77450</td>\n",
       "      <td>7184.644337</td>\n",
       "      <td>3</td>\n",
       "      <td>Friday</td>\n",
       "      <td>11711.968</td>\n",
       "      <td>9741.581</td>\n",
       "      <td>1050</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135567 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  id_reseller       bill  mean-last-30  mean-last-7  \\\n",
       "3      2019-01-31    499921276  10515.048   9249.553500   9249.55350   \n",
       "4      2019-02-01    499921276      0.000   9249.553500   9249.55350   \n",
       "5      2019-02-02    499921276      0.000   9249.553500   9249.55350   \n",
       "6      2019-02-03    499921276      0.000   9249.553500   9249.55350   \n",
       "7      2019-02-04    499921276      0.000   9249.553500  10515.04800   \n",
       "...           ...          ...        ...           ...          ...   \n",
       "139880 2019-05-13    500800672   9741.581  11722.998800  12917.17375   \n",
       "139881 2019-05-14    500800672      0.000  11722.998800  12795.63600   \n",
       "139882 2019-05-15    500800672      0.000  11348.496444  12795.63600   \n",
       "139883 2019-05-16    500800672      0.000  11348.496444   5469.28500   \n",
       "139884 2019-05-17    500800672  11711.968  11384.843600  10726.77450   \n",
       "\n",
       "        std-last-30  days_without_purchase    weekday  next_bill  last_bill  \\\n",
       "3       1789.679485                      2   Thursday  10515.048   7984.059   \n",
       "4       1789.679485                      0     Friday  10717.103  10515.048   \n",
       "5       1789.679485                      1   Saturday  10717.103  10515.048   \n",
       "6       1789.679485                      2     Sunday  10717.103  10515.048   \n",
       "7       1789.679485                      3     Monday  10717.103  10515.048   \n",
       "...             ...                    ...        ...        ...        ...   \n",
       "139880  7280.688377                      2     Monday   9741.581   1196.989   \n",
       "139881  7280.688377                      0    Tuesday  11711.968   9741.581   \n",
       "139882  7619.490861                      1  Wednesday  11711.968   9741.581   \n",
       "139883  7619.490861                      2   Thursday  11711.968   9741.581   \n",
       "139884  7184.644337                      3     Friday  11711.968   9741.581   \n",
       "\n",
       "        zone cluster  \n",
       "3       1050       B  \n",
       "4       1050       B  \n",
       "5       1050       B  \n",
       "6       1050       B  \n",
       "7       1050       B  \n",
       "...      ...     ...  \n",
       "139880  1050       A  \n",
       "139881  1050       A  \n",
       "139882  1050       A  \n",
       "139883  1050       A  \n",
       "139884  1050       A  \n",
       "\n",
       "[135567 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df_r' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store df_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the last 4 months of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = df['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = max_date - pd.to_timedelta(120, unit='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['date'] > min_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engeneering based on past history\n",
    "\n",
    "For each one of the arround 12K resellers, we are going to compute features based on their shopping history: mean and standard deviation of the last week and the last month, and also the amount of the last purchase and how many days as he or she spent without shopping to the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeItem(dfItem):\n",
    "    min_date = dfItem['date'].min()\n",
    "    max_date = dfItem['date'].max()\n",
    "    if min_date == max_date:\n",
    "        #only one data point\n",
    "        return\n",
    "    r = pd.date_range(start=min_date, end=max_date)\n",
    "    dfItemNew = dfItem.set_index('date').reindex(r).rename_axis('date').reset_index()\n",
    "    \n",
    "    dfItemNew['mean-last-30'] = dfItemNew['bill'].rolling(30,min_periods=1).mean().reset_index()['bill']\n",
    "    dfItemNew['mean-last-7'] = dfItemNew['bill'].rolling(7,min_periods=1).mean().reset_index()['bill']\n",
    "    dfItemNew['std-last-30'] = dfItemNew['bill'].rolling(30,min_periods=1).std().reset_index()['bill']\n",
    "    dfItemNew['bill'] = dfItemNew['bill'].fillna(0)\n",
    "    dfItemNew['id_reseller'] = dfItem['id_reseller'].max()\n",
    "    dfItemNew['std-last-30'].fillna(method='ffill',inplace=True)\n",
    "    dfItemNew['mean-last-7'].fillna(method='ffill',inplace=True)\n",
    "    resp = []\n",
    "    counter = 0\n",
    "    for index,row in dfItemNew.iterrows(): \n",
    "        resp.append(counter)\n",
    "        if row['bill'] == 0: \n",
    "            counter += 1 \n",
    "        else:\n",
    "            counter = 0\n",
    "    dfItemNew['days_without_purchase'] = pd.Series(resp)\n",
    "    return dfItemNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200 resellers\n",
      "processed 400 resellers\n",
      "processed 600 resellers\n",
      "processed 800 resellers\n",
      "processed 1000 resellers\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "dfCompletedList = []\n",
    "for nid,item in df.groupby('id_reseller'):\n",
    "    i = i+1\n",
    "    if i%200 == 0:\n",
    "        print ('processed {} resellers'.format(str(i)))\n",
    "    dfCompletedList.append(completeItem(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfCompletedList).copy()\n",
    "del dfCompletedList\n",
    "df['weekday']  = df['date'].dt.weekday_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute next bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['next_bill'] = df.replace(0,np.nan).groupby('id_reseller')['bill'].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute last bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['last_bill'] = df.replace(0,np.nan).groupby('id_reseller')['bill'].fillna(method='ffill').copy()\n",
    "different_zero = df['last_bill'].shift(1)\n",
    "df.loc[df['bill'] != 0,'last_bill'] = np.nan\n",
    "df['last_bill'] = df['last_bill'].fillna(different_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the sales with the rest of the reseller info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_r,how='inner',on='id_reseller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with categorical variables\n",
    "\n",
    "To deal with categorical variables (reseller's cluster and reseller's zone), we will use a combination of sklearn's Label Encoder, a preprocessing module that transforms strings in numeric lables, and One Hot Encoder, that takes this numerical variables and creates dummy (0/1 state) variables. \n",
    "\n",
    "This modules are python objects that keep in their internal variables the information necessary to transform new data.  So, in the Glue ETL we are going to store this objects in pkl format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1019    25569\n",
       "1050    13174\n",
       "1031    10392\n",
       "1033     7007\n",
       "1067     5590\n",
       "1051     4818\n",
       "1034     4402\n",
       "1015     4070\n",
       "Name: zone, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['zone'].value_counts().head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zone'] = df['zone'].apply(lambda x: x if x in [1019,1050,1031,1033,1051,1067] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_cluster = LabelEncoder()\n",
    "ohe_cluster = OneHotEncoder(handle_unknown='ignore')\n",
    "df_cluster = pd.DataFrame(ohe_cluster.fit_transform(le_cluster.fit_transform(df['cluster'].fillna('')).reshape(-1, 1)).todense())\n",
    "df_cluster = df_cluster.add_prefix('cluster_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_zone = LabelEncoder()\n",
    "ohe_zone = OneHotEncoder(handle_unknown='ignore')\n",
    "df_zone = pd.DataFrame(ohe_zone.fit_transform(le_zone.fit_transform(df['zone'].fillna('')).reshape(-1, 1)).todense())\n",
    "df_zone = df_zone.add_prefix('zone_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_weekday = LabelEncoder()\n",
    "ohe_weekday = OneHotEncoder(handle_unknown='ignore')\n",
    "df_weekday = pd.DataFrame(ohe_weekday.fit_transform(le_weekday.fit_transform(df['weekday']).reshape(-1, 1)).todense())\n",
    "df_weekday = df_weekday.add_prefix('weekday_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('s3')\n",
    "client.put_object(Body=pickle.dumps(le_cluster), Bucket=bucket, Key='preprocessing/le_cluster.pkl');\n",
    "client.put_object(Body=pickle.dumps(ohe_cluster), Bucket=bucket, Key='preprocessing/ohe_cluster.pkl')\n",
    "client.put_object(Body=pickle.dumps(le_zone), Bucket=bucket, Key='preprocessing/le_zone.pkl')\n",
    "client.put_object(Body=pickle.dumps(ohe_zone), Bucket=bucket, Key='preprocessing/ohe_zone.pkl')\n",
    "client.put_object(Body=pickle.dumps(le_weekday), Bucket=bucket, Key='preprocessing/le_weekday.pkl')\n",
    "client.put_object(Body=pickle.dumps(ohe_weekday), Bucket=bucket, Key='preprocessing/ohe_weekday.pkl');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to S3 resulting ETL\n",
    "\n",
    "Now we have to write to S3 all the relevant columns. We will perform a train/validation split of the customers so we can train on a group and get relevant metrics on the other.\n",
    "\n",
    "The <b>label</b> that we have to predict in this problem is the amount of the next bill. \n",
    "SageMaker requires, that the input information in csv format contains <b> no header and no index, and that the first column must be occupied by the label. </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['next_bill', 'bill', 'date', 'id_reseller', 'mean-last-30', 'mean-last-7',\n",
    "       'std-last-30', 'days_without_purchase', 'weekday', \n",
    "       'last_bill', 'zone', 'cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,df_cluster,df_zone,df_weekday],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a random 10% sample of the resellers and then shuffle the records \n",
    "val_resellers = list(pd.Series(df['id_reseller'].unique()).sample(frac=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[~df['id_reseller'].isin(val_resellers)].sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = df[df['id_reseller'].isin(val_resellers)].sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['next_bill', 'bill', 'date', 'id_reseller', 'mean-last-30',\n",
       "       'mean-last-7', 'std-last-30', 'days_without_purchase', 'weekday',\n",
       "       'last_bill', 'zone', 'cluster', 'cluster_0', 'cluster_1', 'cluster_2',\n",
       "       'cluster_3', 'cluster_4', 'zone_0', 'zone_1', 'zone_2', 'zone_3',\n",
       "       'zone_4', 'zone_5', 'zone_6', 'weekday_0', 'weekday_1', 'weekday_2',\n",
       "       'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model to be useful, we have to drop all the columns that are wither too specific or that are not going to be available on prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['date','id_reseller','bill','zone','cluster','weekday'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.drop(['date','id_reseller','bill','zone','cluster','weekday'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['next_bill', 'mean-last-30', 'mean-last-7', 'std-last-30',\n",
       "       'days_without_purchase', 'last_bill', 'cluster_0', 'cluster_1',\n",
       "       'cluster_2', 'cluster_3', 'cluster_4', 'zone_0', 'zone_1', 'zone_2',\n",
       "       'zone_3', 'zone_4', 'zone_5', 'zone_6', 'weekday_0', 'weekday_1',\n",
       "       'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pred_columns' (list)\n"
     ]
    }
   ],
   "source": [
    "pred_columns = list(df_train.columns)[1:]\n",
    "%store pred_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.to_csv('validation.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local pickle  preprocessing\n",
    "\n",
    "Now we pickle the sklearn's preprocessing blocks. Later we are going to use them as part of the pipeline, to transform new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_list = [le_cluster,ohe_cluster,le_zone,ohe_zone,le_weekday,ohe_weekday]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessing.pkl', 'wb') as handle:\n",
    "    pickle.dump(preprocessing_list, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
